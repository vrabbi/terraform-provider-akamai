---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "akamai_datastream Resource - terraform-provider-akamai"
subcategory: ""
description: |-
  
---

# akamai_datastream (Resource)





<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `active` (Boolean) Defining if stream should be active or not
- `contract_id` (String) Identifies the contract that has access to the product
- `dataset_fields` (List of Number) A list of data set fields selected from the associated template that the stream monitors in logs. The order of the identifiers define how the value for these fields appear in the log lines
- `delivery_configuration` (Block Set, Min: 1, Max: 1) Provides information about the configuration related to logs (format, file names, delivery frequency) (see [below for nested schema](#nestedblock--delivery_configuration))
- `group_id` (String) Identifies the group that has access to the product and for which the stream configuration was created
- `properties` (List of String) Identifies the properties monitored in the stream
- `stream_name` (String) The name of the stream

### Optional

- `azure_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--azure_connector))
- `collect_midgress` (Boolean) Identifies if stream needs to collect midgress data
- `datadog_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--datadog_connector))
- `elasticsearch_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--elasticsearch_connector))
- `gcs_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--gcs_connector))
- `https_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--https_connector))
- `loggly_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--loggly_connector))
- `new_relic_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--new_relic_connector))
- `notification_emails` (List of String) List of email addresses where the system sends notifications about activations and deactivations of the stream
- `oracle_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--oracle_connector))
- `s3_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--s3_connector))
- `splunk_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--splunk_connector))
- `sumologic_connector` (Block Set, Max: 1) (see [below for nested schema](#nestedblock--sumologic_connector))
- `timeouts` (Block, Optional) (see [below for nested schema](#nestedblock--timeouts))

### Read-Only

- `created_by` (String) The username who created the stream
- `created_date` (String) The date and time when the stream was created
- `id` (String) The ID of this resource.
- `latest_version` (Number) Identifies the latest active configuration version of the stream
- `modified_by` (String) The username who modified the stream
- `modified_date` (String) The date and time when the stream was modified
- `papi_json` (String) The configuration in JSON format that can be copy-pasted into PAPI configuration to enable datastream behavior
- `product_id` (String) The ID of the product for which the stream was created
- `stream_version` (Number) Identifies the configuration version of the stream

<a id="nestedblock--delivery_configuration"></a>
### Nested Schema for `delivery_configuration`

Required:

- `format` (String) The format in which logs will be received
- `frequency` (Block Set, Min: 1, Max: 1) The frequency of collecting logs from each uploader and sending these logs to a destination (see [below for nested schema](#nestedblock--delivery_configuration--frequency))

Optional:

- `field_delimiter` (String) A delimiter that you use to separate data set fields in log lines
- `upload_file_prefix` (String) The prefix of the log file that will be send to a destination
- `upload_file_suffix` (String) The suffix of the log file that will be send to a destination

<a id="nestedblock--delivery_configuration--frequency"></a>
### Nested Schema for `delivery_configuration.frequency`

Required:

- `interval_in_secs` (Number) The time in seconds after which the system bundles log lines into a file and sends it to a destination



<a id="nestedblock--azure_connector"></a>
### Nested Schema for `azure_connector`

Required:

- `access_key` (String, Sensitive) Access keys associated with Azure Storage account
- `account_name` (String) Specifies the Azure Storage account name
- `container_name` (String) Specifies the Azure Storage container name
- `display_name` (String) The name of the connector
- `path` (String) The path to the folder within Azure Storage container where logs will be stored

Read-Only:

- `compress_logs` (Boolean) Indicates whether the logs should be compressed


<a id="nestedblock--datadog_connector"></a>
### Nested Schema for `datadog_connector`

Required:

- `auth_token` (String, Sensitive) The API key associated with Datadog account
- `display_name` (String) The name of the connector
- `endpoint` (String) The Datadog endpoint where logs will be stored

Optional:

- `compress_logs` (Boolean) Indicates whether the logs should be compressed
- `service` (String) The service of the Datadog connector
- `source` (String) The source of the Datadog connector
- `tags` (String) The tags of the Datadog connector


<a id="nestedblock--elasticsearch_connector"></a>
### Nested Schema for `elasticsearch_connector`

Required:

- `display_name` (String) The name of the connector.
- `endpoint` (String) The Elasticsearch bulk endpoint URL in the https://hostname.elastic-cloud.com:9243/_bulk/ format. Set indexName in the appropriate field instead of providing it in the URL. You can use Akamaized property hostnames as endpoint URLs. See Stream logs to Elasticsearch.
- `index_name` (String, Sensitive) The index name of the Elastic cloud where you want to store log files.
- `password` (String, Sensitive) The Elasticsearch basic access authentication password.
- `user_name` (String, Sensitive) The Elasticsearch basic access authentication username.

Optional:

- `ca_cert` (String, Sensitive) The certification authority (CA) certificate used to verify the origin server's certificate. If the certificate is not signed by a well-known certification authority, enter the CA certificate in the PEM format for verification.
- `client_cert` (String, Sensitive) The PEM-formatted digital certificate you want to authenticate requests to your destination with. If you want to use mutual authentication, you need to provide both the client certificate and the client key.
- `client_key` (String, Sensitive) The private key in the non-encrypted PKCS8 format you want to use to authenticate with the backend server. If you want to use mutual authentication, you need to provide both the client certificate and the client key.
- `content_type` (String) The type of the resource passed in the request's custom header. For details, see Additional options in the DataStream user guide.
- `custom_header_name` (String) A human-readable name for the request's custom header, containing only alphanumeric, dash, and underscore characters. For details, see Additional options in the DataStream user guide.
- `custom_header_value` (String) The custom header's contents passed with the request that contains information about the client connection. For details, see Additional options in the DataStream user guide.
- `tls_hostname` (String) The hostname that verifies the server's certificate and matches the Subject Alternative Names (SANs) in the certificate. If not provided, DataStream fetches the hostname from the endpoint URL.

Read-Only:

- `m_tls` (Boolean) Indicates whether mTLS is enabled or not.


<a id="nestedblock--gcs_connector"></a>
### Nested Schema for `gcs_connector`

Required:

- `bucket` (String) The name of the storage bucket created in Google Cloud account
- `display_name` (String) The name of the connector
- `private_key` (String, Sensitive) The contents of the JSON private key generated and downloaded in Google Cloud Storage account
- `project_id` (String) The unique ID of Google Cloud project
- `service_account_name` (String) The name of the service account with the storage.object.create permission or Storage Object Creator role

Optional:

- `path` (String) The path to the folder within Google Cloud bucket where logs will be stored

Read-Only:

- `compress_logs` (Boolean) Indicates whether the logs should be compressed


<a id="nestedblock--https_connector"></a>
### Nested Schema for `https_connector`

Required:

- `authentication_type` (String) Either NONE for no authentication, or BASIC for username and password authentication
- `display_name` (String) The name of the connector
- `endpoint` (String) URL where logs will be stored

Optional:

- `ca_cert` (String, Sensitive) The certification authority (CA) certificate used to verify the origin server's certificate. If the certificate is not signed by a well-known certification authority, enter the CA certificate in the PEM format for verification.
- `client_cert` (String, Sensitive) The digital certificate in the PEM format you want to use to authenticate requests to your destination. If you want to use mutual authentication, you need to provide both the client certificate and the client key (in the PEM format).
- `client_key` (String, Sensitive) The private key in the non-encrypted PKCS8 format you want to use to authenticate with the back-end server. If you want to use mutual authentication, you need to provide both the client certificate and the client key.
- `compress_logs` (Boolean) Indicates whether the logs should be compressed
- `content_type` (String) Content type to pass in the log file header
- `custom_header_name` (String) The name of custom header passed with the request to the destination
- `custom_header_value` (String) The custom header's contents passed with the request to the destination
- `password` (String, Sensitive) Password set for custom HTTPS endpoint for authentication
- `tls_hostname` (String) The hostname that verifies the server's certificate and matches the Subject Alternative Names (SANs) in the certificate. If not provided, DataStream fetches the hostname from the endpoint URL.
- `user_name` (String, Sensitive) Username used for authentication

Read-Only:

- `m_tls` (Boolean) Indicates whether mTLS is enabled or not.


<a id="nestedblock--loggly_connector"></a>
### Nested Schema for `loggly_connector`

Required:

- `auth_token` (String, Sensitive) The unique HTTP code for your Loggly bulk endpoint.
- `display_name` (String) The name of the connector.
- `endpoint` (String) The Loggly bulk endpoint URL in the https://hostname.loggly.com/bulk/ format. Set the endpoint code in the authToken field instead of providing it in the URL. You can use Akamaized property hostnames as endpoint URLs. See Stream logs to Loggly.

Optional:

- `content_type` (String) The type of the resource passed in the request's custom header. For details, see Additional options in the DataStream user guide.
- `custom_header_name` (String) A human-readable name for the request's custom header, containing only alphanumeric, dash, and underscore characters. For details, see Additional options in the DataStream user guide.
- `custom_header_value` (String) The custom header's contents passed with the request that contains information about the client connection. For details, see Additional options in the DataStream user guide.
- `tags` (String) The tags you can use to segment and filter log events in Loggly. See Tags in the Loggly documentation.


<a id="nestedblock--new_relic_connector"></a>
### Nested Schema for `new_relic_connector`

Required:

- `auth_token` (String, Sensitive) Your Log API token for your account in New Relic.
- `display_name` (String) The name of the connector.
- `endpoint` (String) A New Relic endpoint URL you want to send your logs to. The endpoint URL should follow the https://<newrelic.com>/log/v1/ format format. See Introduction to the Log API https://docs.newrelic.com/docs/logs/log-api/introduction-log-api/ if you want to retrieve your New Relic endpoint URL.

Optional:

- `content_type` (String) The type of the resource passed in the request's custom header. For details, see Additional options in the DataStream user guide.
- `custom_header_name` (String) A human-readable name for the request's custom header, containing only alphanumeric, dash, and underscore characters. For details, see Additional options in the DataStream user guide.
- `custom_header_value` (String) The custom header's contents passed with the request that contains information about the client connection. For details, see Additional options in the DataStream user guide.


<a id="nestedblock--oracle_connector"></a>
### Nested Schema for `oracle_connector`

Required:

- `access_key` (String, Sensitive) The access key identifier used to authenticate requests to the Oracle Cloud account
- `bucket` (String) The name of the Oracle Cloud Storage bucket
- `display_name` (String) The name of the connector
- `namespace` (String) The namespace of Oracle Cloud Storage account
- `path` (String) The path to the folder within your Oracle Cloud Storage bucket where logs will be stored
- `region` (String) The Oracle Cloud Storage region where bucket resides
- `secret_access_key` (String, Sensitive) The secret access key identifier used to authenticate requests to the Oracle Cloud account

Read-Only:

- `compress_logs` (Boolean) Indicates whether the logs should be compressed


<a id="nestedblock--s3_connector"></a>
### Nested Schema for `s3_connector`

Required:

- `access_key` (String, Sensitive) The access key identifier used to authenticate requests to the Amazon S3 account
- `bucket` (String) The name of the Amazon S3 bucket
- `display_name` (String) The name of the connector
- `path` (String) The path to the folder within Amazon S3 bucket where logs will be stored
- `region` (String) The AWS region where Amazon S3 bucket resides
- `secret_access_key` (String, Sensitive) The secret access key identifier used to authenticate requests to the Amazon S3 account

Read-Only:

- `compress_logs` (Boolean) Indicates whether the logs should be compressed


<a id="nestedblock--splunk_connector"></a>
### Nested Schema for `splunk_connector`

Required:

- `display_name` (String) The name of the connector
- `endpoint` (String) The raw event Splunk URL where logs will be stored
- `event_collector_token` (String, Sensitive) The Event Collector token associated with Splunk account

Optional:

- `ca_cert` (String, Sensitive) The certification authority (CA) certificate used to verify the origin server's certificate. If the certificate is not signed by a well-known certification authority, enter the CA certificate in the PEM format for verification.
- `client_cert` (String, Sensitive) The digital certificate in the PEM format you want to use to authenticate requests to your destination. If you want to use mutual authentication, you need to provide both the client certificate and the client key (in the PEM format).
- `client_key` (String, Sensitive) The private key in the non-encrypted PKCS8 format you want to use to authenticate with the back-end server. If you want to use mutual authentication, you need to provide both the client certificate and the client key.
- `compress_logs` (Boolean) Indicates whether the logs should be compressed
- `custom_header_name` (String) The name of custom header passed with the request to the destination
- `custom_header_value` (String) The custom header's contents passed with the request to the destination
- `tls_hostname` (String) The hostname that verifies the server's certificate and matches the Subject Alternative Names (SANs) in the certificate. If not provided, DataStream fetches the hostname from the endpoint URL.

Read-Only:

- `m_tls` (Boolean) Indicates whether mTLS is enabled or not.


<a id="nestedblock--sumologic_connector"></a>
### Nested Schema for `sumologic_connector`

Required:

- `collector_code` (String, Sensitive) The unique HTTP collector code of Sumo Logic endpoint
- `display_name` (String) The name of the connector
- `endpoint` (String) The Sumo Logic collection endpoint where logs will be stored

Optional:

- `compress_logs` (Boolean) Indicates whether the logs should be compressed
- `content_type` (String) Content type to pass in the log file header
- `custom_header_name` (String) The name of custom header passed with the request to the destination
- `custom_header_value` (String) The custom header's contents passed with the request to the destination


<a id="nestedblock--timeouts"></a>
### Nested Schema for `timeouts`

Optional:

- `default` (String)
